{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Modelling User Study 2016\n",
    "\n",
    "This notebook will contain my attempts at fitting certain models to the user study as I see fit. This is still largely exploratory, and depending on how the model works to explain some of the work we would like to do, I will refine the product further when I can. \n",
    "\n",
    "I plan for this notebook to contain parametric and non-parametric models. This means I will assume functional form of some models (using methods like Linear Discriminant Analysis, Regression), and non-parametric analysis (Clustering, K-Nearest Neighbors). The motivation for this is to see if there are insights we can draw from the user study analysis. Parametric models are interpretable. Since I assign variables to study, and the model churns out values for the parameters, I can interpret the relationships between certain parameters and gain some knowledge to use moving forward. \n",
    "\n",
    "Non-parametric methods let the machines do that for me. I can narrow down the parameters to study, and let the machine classify users based on their characteristics. For example, if I want to know whether there are distinct usage patterns depending on the area of work the user belongs to, a clustering algorithm can determine that. From there, we can associate usage patterns with area of work.\n",
    "\n",
    "Let's get to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#I will import sci-kit learn procedures as I go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing the data file.\n",
    "data_path  = r\"/Users/Owner/Documents/Work_transfer/User Study 2016/\"\n",
    "\n",
    "df = pd.read_csv(data_path+\"Clean_File.csv\", encoding = \"cp1252\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm gonna do a split-apply-combine method of cleaning the data up a little bit. This just makes like a little easier for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfhelp = df.filter(regex = \"Help\")\n",
    "helpcategories = []\n",
    "\n",
    "for col in dfhelp.columns:\n",
    "    helpcategories.append(dfhelp[col].unique())\n",
    "    \n",
    "helpcategories = list(helpcategories[1]) # This has everything we need\n",
    "helpcategories.remove(helpcategories[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "helpcatdict = {}\n",
    "for cat in helpcategories:\n",
    "    helpcatdict[cat] = cat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "helpcatdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfhelp = dfhelp.replace(helpcatdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfhelp.apply(lambda x: x.astype('category', ordered = True)) # Help is now categorical\n",
    "dfhelp['Participant'] = df['Participant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfwork = df[df.columns[4:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfwork = dfwork.apply(lambda x: x.fillna(0)) # That was easy.\n",
    "dfwork['Participant'] = df['Participant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfno = df.filter(regex = \"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfno.drop(dfno.columns[-3:], inplace = True, axis = 1)\n",
    "dfno.drop('NoArrangement', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfno = dfno.fillna(0) # Swag\n",
    "dfno['Participant'] = df['Participant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfwhy = df.filter(regex = \"Why\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfwhy.drop('NoUseWhy', axis = 1, inplace = True)\n",
    "dfwhy.drop('WhyNotEasy', axis = 1, inplace = True)\n",
    "dfwhy.drop('WhyUseReason', axis = 1, inplace = True)\n",
    "dfwhy = dfwhy.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfwhy #Ill, NOT sick\n",
    "dfwhy['Participant'] = df['Participant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfeasy = df.filter(regex = \"Easy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfeasy.drop('WhyNotEasy', inplace = True, axis = 1)\n",
    "dfeasy.drop('EasyOther', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfeasy = dfeasy.apply(lambda x: np.where(x == \"Yes\", 1, 0)) # I filled Na with this too.\n",
    "dfeasy['Participant'] = df['Participant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfother = df[['Participant', 'Department', 'Community', 'Language', 'Gender']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfother['Department'] = dfother['Department'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfother['Community'] = dfother['Community'].astype('category')\n",
    "dfother['Language'] = dfother['Language'].astype('category')\n",
    "dfother['Gender'] = dfother['Gender'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfother.dtypes #This is what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfmodel = pd.merge(dfother, dfeasy, on = 'Participant').merge(dfwhy, on = 'Participant').merge(dfno, on = 'Participant').merge(dfwork, on = 'Participant').merge(dfhelp, on = 'Participant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apparently there is an existing bug with categories, that they lose their category ranking upon a merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfmodel['Department'] = pd.Categorical(dfmodel['Department'], ordered = False)\n",
    "dfmodel['Community'] = pd.Categorical(dfmodel['Community'], ordered = False)\n",
    "dfmodel['Language'] = pd.Categorical(dfmodel['Language'], ordered = False)\n",
    "dfmodel['Gender'] = pd.Categorical(dfmodel['Gender'], ordered = False)\n",
    "dfmodel['Helpopen'] = pd.Categorical(dfmodel['Helpopen'], ordered = True)\n",
    "dfmodel['HelpAgile'] = pd.Categorical(dfmodel['HelpAgile'], ordered = True)\n",
    "dfmodel['HelpCollab'] =pd.Categorical(dfmodel['HelpCollab'], ordered = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in dfmodel.columns:\n",
    "    if dfmodel[col].dtype == 'float64':\n",
    "        dfmodel[col] = dfmodel[col].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have a clean dataset the makes me happy.\n",
    "\n",
    "The categorization of some of the variables allows for a much cleaner regression sequence. I likely won't use all of these variables, but they're all there if I want to do anything with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold = np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (np.array(dfmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfmodel.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftrain = dfmodel[['Community', 'Department', 'Language', 'Gender', 'WhyUseConnect', 'WhyUsePlan', 'WhyUseCoCreate',\n",
    "                   'WhyUseFeedback', 'WhyUseOrgShareInfo', 'WhyUseFindReUseInfo', 'WhyUseOfficialContent', 'WhyUseFindNewPos',\n",
    "                   'WhyUseCareerDev', 'WhyUseChat']]\n",
    "\n",
    "dfpredict = dfmodel['EasyUse']\n",
    "\n",
    "dfarray = np.array(dftrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.array(dftrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "commdummies = pd.get_dummies(dfmodel['Community'])\n",
    "depdummies = pd.get_dummies(dfmodel['Department'])\n",
    "langdummies = pd.get_dummies(dfmodel['Language'])\n",
    "gendummies = pd.get_dummies(dfmodel['Gender'])\n",
    "\n",
    "joineddf = commdummies.join(depdummies).join(langdummies).join(gendummies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joinedmodel = joineddf.join(dfmodel[['WhyUseConnect', 'WhyUsePlan', 'WhyUseCoCreate',\n",
    "                   'WhyUseFeedback', 'WhyUseOrgShareInfo', 'WhyUseFindReUseInfo', 'WhyUseOfficialContent', 'WhyUseFindNewPos',\n",
    "                   'WhyUseCareerDev', 'WhyUseChat']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joinedmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "I'm going to see if I can feasibly run a nearest neighbour classifier on this data. A strong prediction score would lead to us knowing there are strong patterns present. Unfortunately, the nearest neighbors score is non-parametric, which means I won't really be able to see what the relationship is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "for n_neighbors in range(1,26):    \n",
    "    for weights in ['uniform', 'distance']:\n",
    "        clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights) # Ahh cool it's changing the classificaiton in a loop!\n",
    "        clf.fit(joinedmodel, dfmodel['EasyUse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jma = np.array(joinedmodel) #jma = joinedmodelarray\n",
    "euma = np.array(dfmodel['EasyUse']) #euma = easyusemodelarray\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indices = np.random.permutation(len(euma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll use 4000 to train, 1000 to test and see if this works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jma_train = jma[indices[:-860]]\n",
    "euma_train = euma[indices[:-860]]\n",
    "jma_test = jma[indices[-860:]]\n",
    "euma_test = euma[indices[-860:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "score_dict = {}\n",
    "for n in np.arange(1, 50):\n",
    "    \n",
    "    for weights in ['uniform', 'distance']:\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = n, weights = weights)\n",
    "        \n",
    "        score_dict[n, weights] = knn.fit(jma_train, euma_train).score(jma_test, euma_test)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\" \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfdict = {}\n",
    "for n in np.arange(500,2500, 500):\n",
    "    rf = RandomForestClassifier(n_estimators = n)\n",
    "        \n",
    "    rfdict[n] = rf.fit(jma_train, euma_train).score(jma_test, euma_test)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rfdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've run the Random Forest Classification in both Python and R now, using different specifications. I used the R model for better interpretability, meaning I used fewer parameters, and they were formatted more nicely. I did have a very bad score however (~49%)\n",
    "\n",
    "The Python model has a much higher predictability, however I have a hard time interpreting what it means. It has many more variables, and I suspect many of them are highly correlated.\n",
    "\n",
    "\n",
    "There are a couple things that I need to double check before I go on with any of this. I filled in missing parameters with 0's. Effectively, I prescribed meaning where there effectively was none. These two models are effectively very different because of the assumption that I made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "euma.sum()/len(euma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Sci-kit learn runs into dimensionality problems when one-hot encoding many categorical variables, due to its inability to honour categories.\n",
    "\n",
    "https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/ \n",
    "\n",
    "While, for the moment, things worked out nicely, in the future there are some strategies to reduce this:\n",
    " - R's randomForest package honours categorical variables automatically, so long as the variable doesn't have high cardinality swithcing to the randomForest package may be the least painful way of running the model\n",
    " - Try to find highly related categories. This might be through the data (Principal Component Analysis, which I need to brush up on), or through making predetermined presuppositions. This might be useful in getting the information I need.\n",
    " - Even in R, having a vast amount of categorical variables can drastically reduce performance.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Random Forest Model\n",
    "\n",
    "In the interest of transparency of my efforts/lessons learned, I'm keeping all my work here. Below is the final work that I've since completed. I originally fit the model in Spyder, and have since imported it into this notebook.\n",
    "\n",
    "In both the models in R and Python, I've had issues with the high cardinality of regions and communities and departments. When I tried to fit the model to the large amount of variables, I ran into very low scores. The scores in this data do fairly well, which is nice.\n",
    "\n",
    "While I found many resources identifying the difficulties of high-cardinality variables in scikit learn, (and R too), there hasn't been any resource to suggest a remedy to this situation. I can't code \n",
    "I just dropped the variable, as there are too many options for the amount of data we have left anyway, it gives bad responses.\n",
    "\n",
    "\n",
    "### What is Random Forest?\n",
    "\n",
    "Random forest comes from the concept of decision trees. Decision trees try to make decisions on the label of a particular observation by splitting the observations according to certain properties, and to continue doing so until a series of \"rules\" are created. These decision rules work similarly to how a human would think in a 20 questions game: If you need to guess the word \"tiger,\" you might think something like this:\n",
    "\n",
    "\n",
    "Is the thing a person? No\n",
    "Is it an animal? Yes\n",
    "Is it a mammal? Yes\n",
    "Does it live on land? Yes\n",
    "Does it live in North America? No\n",
    "...\n",
    "..\n",
    "Then it must be a tiger!\n",
    "\n",
    "\n",
    "A decision tree follows that sort of logic but with the data. It tries to classify an unknown observation given what it knows about the data.\n",
    "\n",
    "The issue with decision trees are the high variance, and the low bias in the process. Essentially, the decision tree designs the rules to make its decision around the training data, and can make its rules too complex, overfitting the model. This means that the decision tree is too well tailored to the data it is given. If you give a different training set of the same observations for the machine to construct a tree out of, it will likely construct a different tree. This is known as variance, where the models' results change drastically given a different sample. However since these decision trees can be sophisticated, they are able to construct relationships very accurately. There is not a very high risk in over-simplifying the tree, since computers are really good at capturing the relationships through trees.\n",
    "\n",
    "\n",
    "A random forest helps the variance issue, while trying to keep the low bias advantage in the decision tree. The random forest takes MANY samples from the sample data, and creates many trees (usually over 500 decision trees). From the sample, the random forest takes a random sample of the decision parameters in the model, and constructs a tree from them, the repeats this process many times. Then an average of all the trees is used to make a decision about a node.\n",
    "\n",
    "\n",
    "The averaging process greatly reduces the variance of the model, while still keeping the low bias of the decision tree. It is known as one of the best out-of-the-box classifiers around (although to be fair, many make that claim). Additionally, the way that decision trees make their decisions (out of bag sampling) removes the necessity of N-fold cross-validation. This is handy AND convenient!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path+\"Clean_File.csv\", encoding = \"latin1\") #Reloading the file\n",
    "\n",
    "df.drop([\"Unnamed: 0\", \"Participant\"], inplace = True, axis = 1) # These are useless columns for prediction\n",
    "\n",
    "#Identify columns to drop - either text questions or too-high cardinality\n",
    "dropcols = ['DepartmentOther', 'StatusOther', 'UsageLengthOther',\n",
    "            'OtherResponse', 'WhyUseReason', 'WhyNotEasy', 'FeaturesWant',\n",
    "            'OtherBenefits', 'Tenure', 'EasyOther', 'Department', 'Community', 'Region']\n",
    "            \n",
    "df.drop(dropcols, inplace = True, axis = 1) # These columnds are either textual or have too-high cardinality\n",
    "# I've ran the models with these features in place with R (which allows for categorical variables)\n",
    "# There are too many departments to parse through, and the others blow up feature space \n",
    "\n",
    "\n",
    "for col in df.columns: # In the survey, the 'float64' variables were binary (0 or 1). In the dataframe, it was represented as\n",
    "    if df[col].dtype == 'float64': # (1 or NA) which is bad news bears.\n",
    "        df[col] = df[col].fillna(0) # This fills in the zeroes, but doesn't patch up missing variables.\n",
    "    \n",
    "df.dropna(inplace = True, how = 'any') # Since filling in the zeroes where necessary, any blank spaces are now dropped.\n",
    "\n",
    "\n",
    "\n",
    "df['EasyUse'] = [2 if x == 'Yes' else 1 if x == 'No' else 0 for x in df['EasyUse']]\n",
    "collist = []\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'object':\n",
    "        collist.append(col)\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1) # To split into a random training set.\n",
    " # Splitting the values into Yes No or Dunno\n",
    "\n",
    "df1 = df.drop('EasyUse', axis = 1)\n",
    "df_with_dummies = pd.get_dummies(df1, columns = collist) # Creating the dummy matrix\n",
    "\n",
    "\n",
    "easyuse = np.array(df['EasyUse'])\n",
    "rfmodel =np.array(df_with_dummies) # Looking back, this is almost entirely a sparse matrix, and could have been converted as such\n",
    " # But as far as I know, this would only improve performance of the computer, which isn't very necessary\n",
    "\n",
    "indices = np.random.permutation(len(easyuse)) # The (pseudo)randomly generated list of indices to split the set\n",
    "\n",
    "easyuse_train = easyuse[indices[:-500]] #Almost 2500 observations, so splitting into almost 80:20\n",
    "rfmodel_train = rfmodel[indices[:-500]]\n",
    "\n",
    "easyuse_test = easyuse[indices[-500:]]\n",
    "rfmodel_test = rfmodel[indices[-500:]]\n",
    "\n",
    "\n",
    "\n",
    "rfdict = {} # Running the random forests with different parameters for the number of trees\n",
    " # Note that Random Forest always uses sqrt(p) parameters for \n",
    "for n in np.arange(500,2500, 100):\n",
    "        rf = RandomForestClassifier(n_estimators = n)\n",
    "        \n",
    "        rfdict[n] = rf.fit(rfmodel_train, easyuse_train).score(rfmodel_test, easyuse_test)\n",
    "    \n",
    "# Tells us what the most important features are\n",
    "\n",
    "features_dict = {}\n",
    "for i in range(len(df_with_dummies.columns)):\n",
    "    features_dict[df_with_dummies.columns[i]] = rf.feature_importances_[i]\n",
    "#There we go. This is a strong prediction.\n",
    "\n",
    "\n",
    "features = pd.DataFrame.from_dict(features_dict, orient = 'index')\n",
    "features.sort(0, ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable Importance\n",
    "\n",
    "The above text is the \"Variable Importance\" table of the random forest algorithm. According to Elements of Statistical Learning:\n",
    "\n",
    "\n",
    "\"At each split in each tree, the improvement in the split-criterion is the\n",
    "importance measure attributed to the splitting variable, and is accumulated\n",
    "over all the trees in the forest separately for each variable.\"\n",
    "\n",
    "\n",
    "Essentially, the importance of a variable is based on how much it improves the split-criterion in the tree. If the variable is important in classifying the observation, it will split the data along the tree very well. In the example above, when we tried to guess \"Tiger,\" the variable (\"Animal\") would be important, since it helps the guesser eliminate a lot of the options that the secret word is not. However something like (\"Is this object on Earth?\") would be less important in the classification.\n",
    "\n",
    "We see above in the table that the most important variables are far and away whether individuals find it easy to find information on GCconnex.\n",
    "\n",
    "I ran a random forest in R as well (since it is much better suited to categorical variables than Sci-kit learn), and the strongest predictor of whether an individual finds GCconnex easy to use is whether it is easy or not to find information. Additionally, onboarding was an important feature in determining whether or not they found GCconnex useful.\n",
    "\n",
    "\n",
    "\n",
    "# Weaknesses of this classification:\n",
    "\n",
    "This model isn't perfect, not by a long shot. There are some complications that reduce the effectiveness of the classification:\n",
    "\n",
    "- Variables are related\n",
    "    - Random Forests are best when each variable is unrelated to the other, however in these surveys, this is likely not the case.\n",
    "    - As a result, there is likely higher variance than there should be in using the Random Forest. This means that there is a greater likelihood that the results would change if we were to re-run the survey and re-run the classification.\n",
    "  \n",
    "  \n",
    "- Blown up feature space (likely with irrelevant variables)\n",
    "    - Using many irrelevant variables reduces the likelihood that a tree would randomly take a relevant variable in the classification. When using variables like region, or community or department, the feature space gets blown up, resulting in the more relevant variables being taken up less frequently, reducing effectiveness.\n",
    "    \n",
    "    \n",
    "- Imbalance of features\n",
    "    - In the  \"EasyUse\" parameter, there are very few \"Don't Know / Not Sure\" observations. This greatly reduces the accuracy of these classifications, as is apparent from the Confusion matrix. This may also be evident in the independent parameters as well.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### R Code\n",
    "$ R \n",
    "library(randomForest)\n",
    "library(dplyr)\n",
    "#The dataset has 86 variables, which is quite a bit. Let's see if we can break this down into something better.\n",
    "#The reason why I'm using R for this is because of how straight forward the random Forest package. That and RStudio is my fave.\n",
    "df <- read.csv(\"/Users/Owner/Documents/Work_transfer/User Study 2016/Clean_File.csv\", na.strings = c(\"\", \"NA\"))\n",
    "\n",
    "dropcols <- c(\"DepartmentOther\", \"UsageLengthOther\", \"WhyUseReason\", \"WhyNotEasy\", \"FeaturesWant\", \"OtherBenefits\", \"StatusOther\")\n",
    "\n",
    "df <- df[,!names(df) %in% dropcols] # Dropping the above observations.\n",
    "\n",
    "df <- df[,-c(1,2)] # Dropping unnecessary indices\n",
    "\n",
    "df[,2:7][is.na(df[,2:7])] <- 0 \n",
    "df[,20:28][is.na(df[,20:28])] <- 0\n",
    "\n",
    "\n",
    "df <- subset(df, select = -c(OtherResponse))\n",
    "df <- subset(df, select = -c(EasyOther))            \n",
    "\n",
    "df[,30:40][is.na(df[,30:40])] <- 0\n",
    "\n",
    "#There, that's the worst of it I think.\n",
    "df <- df[complete.cases(df),]\n",
    "\n",
    "df <- df[,-1] # Need to drop department because too many possibilities\n",
    "\n",
    "train <- sample(1:nrow(df) , 1800)\n",
    "\n",
    "df.test <- df[-train,\"EasyUse\"] # Test dataset\n",
    "\n",
    "set.seed(1)\n",
    "\n",
    "\n",
    "\n",
    "rf.easy <- randomForest(EasyUse~.,\n",
    "                         data=df, subset = train, importance = TRUE, ntrees = 2000, proximity = TRUE)\n",
    "rf.easy\n",
    "varImpPlot(rf.easy) #This might actually be fairly useful. EasyInfo is by far the most important predictor of whether they\n",
    " # find GCconnex useful\n",
    "importance(rf.easy)\n",
    "\n",
    "#Plotting the error rates.\n",
    "plot(rf.easy, main = \"Random Forest 'Ease of Use' Prediction Error Rates\", lty = 1)\n",
    "legend(\"right\", colnames(rf.easy$err.rate), col = 1:4, cex = 0.8, fill = 1:4)\n",
    "\n",
    "\n",
    "MDSplot(rf.easy, df$EasyUse)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Check out the data visualization workbook for some visual exploration of the results\n",
    "\n",
    "from sklearn import tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = tree.DecisionTreeClassifier()\n",
    "classifier = classifier.fit(rfmodel_train, easyuse_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dot_data = tree.export_graphviz(classifier, out_file = data_path+\"easy_to_use_tree.dot\",\n",
    "                                feature_names = df_with_dummies.columns, filled = True, rounded = True)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_file(data_path+\"easy_to_use_tree.dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(graph.create_png())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
